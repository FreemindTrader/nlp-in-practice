{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FreemindTrader/nlp-in-practice/blob/master/Chapter_5_Backprop_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ygl7vdLYAD"
      },
      "source": [
        "# Introduction\n",
        "-----------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxIckbJMLavN"
      },
      "source": [
        "\n",
        "In this Notebook, we're going to step through an update of the weight matrices for a skip-gram word2vec model using Negative Sampling. This means we'll be implementing both negative sampling and backprop from scratch!\n",
        "\n",
        "This Notebook has two major parts.\n",
        "\n",
        "### Part 1 - Pre-training with gensim\n",
        "\n",
        "In **Part 1**, we are going to use `gensim` to *begin* to train a word2vec model on the Wikipedia comments dataset. `gensim` will handle the construction of the vocabulary for us, and perform the first two training passes. Two passes is enough such that the word vectors will be reasonable, but still pretty bad. I chose to do this because I think a weight update in this state is more informative than if we did it on either *fully trained* vectors, or *completely random* vectors.\n",
        "\n",
        "*You may __skip reading Part 1__ and __go straight to Part 2__ if you'd like.*\n",
        "\n",
        "### Part 2 - Manual weight update\n",
        "\n",
        "In **Part 2**, we will take a single training word pair (\"thought\", \"well\"), and implement the weight update from scratch. This will require implementing negative sampling (to select the negative samples) and implementing the gradient calculations for updating the word vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5036469LYAF"
      },
      "source": [
        "# Contents\n",
        "-----------------\n",
        "\n",
        "**Part 1 - Pretraining with gensim**\n",
        "* [Dataset Preparation](#Dataset-Preparation)\n",
        "    * [Download the dataset](#Download-the-dataset)\n",
        "    * [Parse the dataset file](#Parse-the-dataset-file)\n",
        "    * [Tokenize the comments](#Tokenize-the-comments)\n",
        "* [Pre-Training](#Training)\n",
        "    * [Configure logging](#Configure-logging)\n",
        "    * [Set model parameters](#Set-model-parameters)\n",
        "    * [Build the vocabulary](#Build-the-vocabulary)\n",
        "    * [Train the model](#Train-the-model)\n",
        "    * [Play with results](#Play-with-results)\n",
        "    \n",
        "**Part 2 - Manual Weight Update**    \n",
        "* [Negative Sampling from Scratch](#Negative-Sampling-from-Scratch)\n",
        "    * [Generating the Unigram Table](#Generating-Unigram-Table)\n",
        "    * [Inspect the Table](#Inspect-the-Table)\n",
        "        * [Row Counts](#Row-Counts)\n",
        "        * [Compare Probability Distributions](#Compare-Probability-Distributions)\n",
        "        * [20 Random Words](#20-Random-Words)\n",
        "    * [Backprop](#Backprop)\n",
        "        * [Retrieve Weight Matrices](#Retrieve-Weight-Matrices)\n",
        "        * [Picking Samples](#Picking-Samples)\n",
        "        * [Weight Updates](#Weight-Updates)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4HdWx5NLYAG"
      },
      "source": [
        "# Part 1 - Pretraining with gensim\n",
        "------------------------------------------------------\n",
        "\n",
        "\n",
        "# Dataset Preparation\n",
        "-----------------------------------\n",
        "In this section we'll download a text dataset comprised of comments on Wikipedia which contain \"attacks\" on other users (plus counter-examples).\n",
        "\n",
        "We'll use `pandas` for CSV parsing and `gensim` for tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVuyA-X3LYAH"
      },
      "source": [
        "## Download the dataset\n",
        "--------------------------------------\n",
        "Download the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF1u3OtGL6Tu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "a9cbaf91-a032-4f03-bb76-6244a8afcfc6"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=79830acb96fb80c2523d64204b187e06a415dffc4b2b1b9eb27f5ad4ea572751\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlMvowf7LYAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d16b798-6c90-4ca1-97eb-b729e928046b"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "# Create the data subdirectory if not there.\n",
        "if not os.path.exists('./data/'):\n",
        "    os.mkdir('./data/')\n",
        "\n",
        "filename = './data/attack_annotated_comments.tsv'\n",
        "\n",
        "# Download download if we already have it!\n",
        "if not os.path.exists(filename):\n",
        "\n",
        "    # URL for the CSV file (~55.4MB) containing the wikipedia comments.\n",
        "    url = 'https://ndownloader.figshare.com/files/7554634'\n",
        "\n",
        "    # Download the dataset.\n",
        "    print('Downloading Wikipedia Attack Comments dataset (~55.4MB)...')\n",
        "    wget.download(url, filename)\n",
        "\n",
        "    print('  DONE.')\n",
        "\n",
        "# We won't use these, but FYI, this is the file containing the labels\n",
        "# for the comments.\n",
        "#   url = 'https://ndownloader.figshare.com/files/7554637'\n",
        "#   filename = './data/attack_annotated_comments.tsv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Wikipedia Attack Comments dataset (~55.4MB...\n",
            "  DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as5XgRaOLYAP"
      },
      "source": [
        "## Parse the dataset file\n",
        "--------------------------------\n",
        "We'll use `pandas` just to help us parse the tab-separated `.tsv` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke1ua64YLYAQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1ee85df1-16a8-444c-f57a-0ded5dfad1e2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('Parsing the dataset .tsv file...')\n",
        "comments = pd.read_csv('./data/attack_annotated_comments.tsv', sep = '\\t')\n",
        "\n",
        "print('    Done.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing the dataset .tsv file...\n",
            "    Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiTEZKpPLYAX"
      },
      "source": [
        "## Tokenize the comments\n",
        "------------------------------------\n",
        "This dataset uses the special labels \"NEWLINE_TOKEN\" and \"TAB_TOKEN\" to represent the newline and tab characters. We'll replace these with a single space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg_cFsM-LYAY"
      },
      "source": [
        "# remove newline and tab tokens\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tziKp3vKLYAd"
      },
      "source": [
        "Next, Use gensim to perform a simple tokenization strategy to the text and turn each comment into a list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT4iB9lgLYAe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "810d8781-e2a1-40e1-a176-7a89a16890fc"
      },
      "source": [
        "%%time\n",
        "\n",
        "import gensim\n",
        "import io\n",
        "\n",
        "print('Tokenizing comments...')\n",
        "\n",
        "# Track the total number of tokens in the dataset.\n",
        "num_tokens = 0\n",
        "\n",
        "# List of sentences to use for training.\n",
        "sentences = []\n",
        "\n",
        "# For each comment...\n",
        "for i, row in comments.iterrows():\n",
        "\n",
        "    # Report progress.\n",
        "    if ((i % 20000) == 0):\n",
        "        print('  Read {:,} comments.'.format(i))\n",
        "\n",
        "    # Tokenize the comment. This returns a list of words.\n",
        "    parsed = gensim.utils.simple_preprocess(row.comment)\n",
        "\n",
        "    # Accumulate the total number of words in the dataset.\n",
        "    num_tokens += len(parsed)\n",
        "\n",
        "    # Add the comment to the list.\n",
        "    sentences.append(parsed)\n",
        "\n",
        "print('DONE.')\n",
        "print('')\n",
        "print('{:>10,} comments'.format(i))\n",
        "print('{:>10,} tokens'.format(num_tokens))\n",
        "print('{:>10,} avg. tokens / comment'.format(int(num_tokens / len(sentences))))\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing comments...\n",
            "  Read 0 comments.\n",
            "  Read 20,000 comments.\n",
            "  Read 40,000 comments.\n",
            "  Read 60,000 comments.\n",
            "  Read 80,000 comments.\n",
            "  Read 100,000 comments.\n",
            "DONE.\n",
            "\n",
            "   115,863 comments\n",
            " 7,651,029 tokens\n",
            "        66 avg. tokens / comment\n",
            "\n",
            "CPU times: user 25 s, sys: 655 ms, total: 25.7 s\n",
            "Wall time: 26.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpiPTHSTLYAk"
      },
      "source": [
        "# Training\n",
        "----------------\n",
        "\n",
        "Time to train the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWWb5mhvLYAl"
      },
      "source": [
        "## Configure Logging\n",
        "-----------------------------\n",
        "`gensim` provides some valuable information about the training process using the `logging` module in Python.\n",
        "\n",
        "In order to see this log output, we first need to setup logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AYEzj65LYAm"
      },
      "source": [
        "import logging\n",
        "\n",
        "# Enable logging at the `INFO` level and set a custom format--the\n",
        "# default log format is pretty wordy.\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s : %(message)s', # Display just time and message.\n",
        "    datefmt='%H:%M:%S', # Display time, but not the date.\n",
        "    level=logging.INFO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fg2_X0QMzpq"
      },
      "source": [
        "Let's also suppress any pesky warnings from libraries that gensim references."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abgzK0kaMxuz"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0UZUdmpLYAq"
      },
      "source": [
        "## Set Model Parameters\n",
        "----------------------------------\n",
        "\n",
        "We define all of the parameters for our model upfront. Take a look at the code comments for each parameter below.\n",
        "\n",
        "Also, for reference:\n",
        "* Documentation for [gensim.models.Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec).\n",
        "* Source code for [gensim.models.Word2Vec](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py#L659) constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkeyndqYLYAs"
      },
      "source": [
        "model = gensim.models.Word2Vec (\n",
        "    size=100,    # Number of features in word vector\n",
        "\n",
        "    window=10,   # Context window size (in each direction)\n",
        "                 #   Default is 5\n",
        "\n",
        "    min_count=2, # Words must appear this many times to be in vocab.\n",
        "                 #   Default is 5\n",
        "\n",
        "    workers=10,  # Training thread count\n",
        "\n",
        "    sg=0,        # 0: CBOW, 1: Skip-gram.\n",
        "                 #   Default is 0, CBOW\n",
        "\n",
        "    hs=0,        # 0: Negative Sampling, 1: Hierarchical Softmax\n",
        "                 #   Default is 0, NS\n",
        "\n",
        "    negative=5   # Nmber of negative samples\n",
        "                 #   Default is 5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juQqn4OKLYAx"
      },
      "source": [
        "## Build the Vocabulary\n",
        "---------------------------------\n",
        "\n",
        "Before we can train the word2vec neural network, we need to create a vocabulary. The vocabulary contains the full list of words that we will end up learning word vectors for.\n",
        "\n",
        "* Source code for `build_vocab` is at [base_any2vec.py#L896](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/base_any2vec.py#L896)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuZ7uamnLYAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "57914376-fc36-4704-9fec-26ae8532846f"
      },
      "source": [
        "# Build the vocabulary using the comments in \"sentences\".\n",
        "model.build_vocab(\n",
        "    sentences, # Our comments dataset\n",
        "    progress_per=20000  # Update after this many sentences.\n",
        "                        # Too many progress updates is annoying!\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22:58:55 : collecting all words and their counts\n",
            "22:58:55 : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "22:58:56 : PROGRESS: at sentence #20000, processed 1399843 words, keeping 51238 word types\n",
            "22:58:56 : PROGRESS: at sentence #40000, processed 2764033 words, keeping 76280 word types\n",
            "22:58:56 : PROGRESS: at sentence #60000, processed 4091968 words, keeping 94720 word types\n",
            "22:58:57 : PROGRESS: at sentence #80000, processed 5354741 words, keeping 112164 word types\n",
            "22:58:57 : PROGRESS: at sentence #100000, processed 6640746 words, keeping 129161 word types\n",
            "22:58:57 : collected 141062 word types from a corpus of 7651029 raw words and 115864 sentences\n",
            "22:58:57 : Loading a fresh vocabulary\n",
            "22:58:58 : effective_min_count=2 retains 71038 unique words (50% of original 141062, drops 70024)\n",
            "22:58:58 : effective_min_count=2 leaves 7581005 word corpus (99% of original 7651029, drops 70024)\n",
            "22:58:58 : deleting the raw counts dictionary of 141062 items\n",
            "22:58:58 : sample=0.001 downsamples 48 most-common words\n",
            "22:58:58 : downsampling leaves estimated 5905950 word corpus (77.9% of prior 7581005)\n",
            "22:58:58 : estimated required memory for 71038 words and 100 dimensions: 92349400 bytes\n",
            "22:58:58 : resetting layer weights\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lxlD1VVLYA2"
      },
      "source": [
        "## Train the model\n",
        "-------------------------\n",
        "\n",
        "Now that we have a vocabulary built, we're ready to train the model.\n",
        "\n",
        "*IMPORTANT: We are only running two training passes (epochs=2) so that we get the weights into a reasonable, but still imperfect state.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrvugJMpLYA3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "e43c2045-8149-4bcd-8b3f-4776ef0d18f4"
      },
      "source": [
        "%%time\n",
        "\n",
        "print('Training the model...')\n",
        "\n",
        "model.train(\n",
        "    sentences,\n",
        "    total_examples=len(sentences),\n",
        "    epochs=2,        # How many training passes to take.\n",
        "    report_delay=10.0 # Report progress every 10 seconds.\n",
        ")\n",
        "\n",
        "print('  Done.')\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22:59:13 : training model with 10 workers on 71038 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training the model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "22:59:14 : EPOCH 1 - PROGRESS: at 5.39% examples, 368547 words/s, in_qsize 19, out_qsize 0\n",
            "22:59:24 : EPOCH 1 - PROGRESS: at 78.82% examples, 425096 words/s, in_qsize 19, out_qsize 0\n",
            "22:59:27 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:59:27 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:59:27 : EPOCH - 1 : training on 7651029 raw words (5906604 effective words) took 13.8s, 427547 effective words/s\n",
            "22:59:28 : EPOCH 2 - PROGRESS: at 5.75% examples, 366071 words/s, in_qsize 20, out_qsize 2\n",
            "22:59:38 : EPOCH 2 - PROGRESS: at 80.59% examples, 430053 words/s, in_qsize 19, out_qsize 0\n",
            "22:59:41 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:59:41 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:59:41 : EPOCH - 2 : training on 7651029 raw words (5907661 effective words) took 13.6s, 435316 effective words/s\n",
            "22:59:41 : training on a 15302058 raw words (11814265 effective words) took 27.4s, 430850 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Done.\n",
            "\n",
            "CPU times: user 53.1 s, sys: 268 ms, total: 53.4 s\n",
            "Wall time: 27.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhvwNkfaLYA7"
      },
      "source": [
        "## Play with results\n",
        "---------------------------\n",
        "As a quick, informal test of the quality of the model, let's look at some word comparisons that we think might appear in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxto2aAqLYA8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "51f06a0a-f26b-4155-ec01-42b445dbe160"
      },
      "source": [
        "model.wv.most_similar('condescending')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('childish', 0.788465142250061),\n",
              " ('rude', 0.7828449606895447),\n",
              " ('arrogant', 0.7772617340087891),\n",
              " ('malleus', 0.7670767903327942),\n",
              " ('aggressive', 0.7646420001983643),\n",
              " ('sarcastic', 0.759050726890564),\n",
              " ('nasty', 0.7552060484886169),\n",
              " ('pedantic', 0.7531112432479858),\n",
              " ('insulting', 0.7473530769348145),\n",
              " ('reprimanded', 0.7345737814903259)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-LRm121LYBA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87048628-b301-483e-832a-1a83d94cdc31"
      },
      "source": [
        "print(model.wv.similarity('stupid', 'dumb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8387435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGYrpSalLYBD"
      },
      "source": [
        "# Part 2 - Manual Weight Update\n",
        "----------------------------------------------------\n",
        "In Part 1, we partially trained a word2vec model using gensim. Now, in Part 2, we're going to implement the training from scratch, and walk through a single training sample in order to see it in detail.\n",
        "\n",
        "We'll take the word pair (\"though\", \"well\"), and make the necessary weight updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5IZPe_ALYBE"
      },
      "source": [
        "# Negative Sampling from Scratch\n",
        "-------------------------------------------------------\n",
        "\n",
        "In order to update the weights properly, we also need to implement Negative Sampling so that we can randomly choose 5 words as negative samples *using the correct probabilities*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvQsgokxLYBF"
      },
      "source": [
        "## Generating Unigram Table\n",
        "----------------------------------------\n",
        "I've implemented negative sampling here by porting the code from the `InitUnigramTable` function in the original [word2vec.c](https://github.com/chrisjmccormick/word2vec_commented/blob/master/word2vec.c).\n",
        "\n",
        "Each word is given a weight equal to its frequency (word count) raised to the 3/4 power. The probability for a selecting a word is just its weight divided by the sum of weights for all words.\n",
        "\n",
        "To implement this, we have a large array, and we fill it with the word ids from our vocabulary. word ids appear multiple times in the table such that `(number of rows with word i) / (table size) = probability of choosing word i`.\n",
        "\n",
        "Every vocab word appears at least once in the table.\n",
        "\n",
        "The size of the table relative to the size of the vocab dictates the resolution of the sampling. A larger unigram table means the negative samples will be selected with a probability that more closely matches the probability calculated by the equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ynvHadHLYBF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "dabcadae-a1f8-4e79-a391-cc3cd64aea8e"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# The original code used a table size of 100M for a vocab of 3M words.\n",
        "# Since our vocab is only ~75K, we'll use 10M instead.\n",
        "table_size = int(10e6)\n",
        "d1 = 0.75\n",
        "power = 0.75\n",
        "\n",
        "# Bonus - What table size should we use to be proportional to the\n",
        "#         original 3M word model?\n",
        "# print('{:,}'.format(int(100e6 / 3e6 * len(entries))))\n",
        "\n",
        "# Allocate the table.\n",
        "uni_table = np.ndarray((table_size,1), dtype='int')\n",
        "\n",
        "# Get all of the vocab entries as a list.\n",
        "entries = model.wv.vocab.values()\n",
        "\n",
        "print('Sorting vocab...')\n",
        "\n",
        "# Sort them by decreasing frequency...\n",
        "entries = sorted(entries, key=lambda entry: entry.count, reverse=True)\n",
        "\n",
        "# Also total up the counts, so we can compare to unigram.\n",
        "total_count = 0\n",
        "train_words_pow = 0\n",
        "\n",
        "print('Accumulating denominator...')\n",
        "\n",
        "# Calculate the denominator, which is the sum of weights for all words.\n",
        "for entry in entries:\n",
        "    train_words_pow += pow(entry.count, power)\n",
        "    total_count += entry.count\n",
        "\n",
        "print('Done.')\n",
        "\n",
        "print('Filling out unigram table...')\n",
        "\n",
        "# 'i' is the vocabulary index of the current word, whereas 'a' will be\n",
        "# the index into the unigram table.\n",
        "i = 0\n",
        "\n",
        "# d1 will store the probability that we choose word `i` as a fraction\n",
        "# between 0 and 1.\n",
        "d1 = pow(entries[i].count, power) / train_words_pow;\n",
        "\n",
        "# Loop over all positions in the table.\n",
        "for a in range(0, table_size):\n",
        "\n",
        "  # Update progress every 1M entries.\n",
        "  if a % int(1e6) == 0:\n",
        "    print('    At table position {:<10,} / {:,}'.format(a, len(uni_table)))\n",
        "\n",
        "  # Store word 'i' in this position. Word 'i' will appear multiple times\n",
        "  # in the table, based on its frequency in the training data.\n",
        "  uni_table[a] = i;\n",
        "\n",
        "  # If the fraction of the table we have filled is greater than the\n",
        "  # probability of choosing this word, then move to the next word.\n",
        "  if (float(a) / float(table_size) > d1):\n",
        "\n",
        "    # Move to the next word.\n",
        "    i += 1;\n",
        "\n",
        "    # Calculate the probability for the new word, and accumulate it with\n",
        "    # the probabilities of all previous words, so that we can compare d1 to\n",
        "    # the percentage of the table that we have filled.\n",
        "    d1 += pow(entries[i].count, power) / train_words_pow\n",
        "\n",
        "  # Don't go past the end of the vocab.\n",
        "  # The total weights for all words should sum up to 1, so there shouldn't\n",
        "  # be any extra space at the end of the table. Maybe it's possible to be\n",
        "  # off by 1, though? Or maybe this is just precautionary.\n",
        "  if (i >= len(entries)):\n",
        "    print('Triggered the end check!')\n",
        "    i = len(entries) - 1;\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sorting vocab...\n",
            "Accumulating denominator...\n",
            "Done.\n",
            "Filling out unigram table...\n",
            "    At table position 0          / 10,000,000\n",
            "    At table position 1,000,000  / 10,000,000\n",
            "    At table position 2,000,000  / 10,000,000\n",
            "    At table position 3,000,000  / 10,000,000\n",
            "    At table position 4,000,000  / 10,000,000\n",
            "    At table position 5,000,000  / 10,000,000\n",
            "    At table position 6,000,000  / 10,000,000\n",
            "    At table position 7,000,000  / 10,000,000\n",
            "    At table position 8,000,000  / 10,000,000\n",
            "    At table position 9,000,000  / 10,000,000\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSoWZOjZLYBJ"
      },
      "source": [
        "## Inspect the Table\n",
        "---------------------------\n",
        "Let's check out a few properties of the table.\n",
        "\n",
        "### Row Counts\n",
        "------------------\n",
        "Each word has a number of spots in the table proportional to its sampling probability, so as a point of reference, let's see how many spots are occupied by the *least common word* and by the *most common word*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4SEFgeFLYBJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "35a23223-6e9d-4815-a542-fcd227acb420"
      },
      "source": [
        "# Get the last word in the table.\n",
        "last_word = uni_table[-1]\n",
        "num_spaces = 0\n",
        "\n",
        "# Loop backwards through the table...\n",
        "for i in range(-1, -len(uni_table), -1):\n",
        "\n",
        "    # Stop when the word changes.\n",
        "    if not uni_table[i] == last_word:\n",
        "        break\n",
        "\n",
        "    num_spaces += 1\n",
        "\n",
        "print('The least common word has {:,} spots'.format(num_spaces))\n",
        "\n",
        "# Look up the first word in the table.\n",
        "first_word = uni_table[0]\n",
        "num_spaces = 0\n",
        "\n",
        "# Loop forward through the table\n",
        "for i in range(0, len(uni_table)):\n",
        "\n",
        "    # Stop when the word changes.\n",
        "    if not uni_table[i] == first_word:\n",
        "        break\n",
        "\n",
        "    num_spaces += 1\n",
        "\n",
        "print('The most common word has {:,} spots'.format(num_spaces))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The least common word has 13 spots\n",
            "The most common word has 130,193 spots\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7v6zyfZLYBM"
      },
      "source": [
        "### Compare Probability Distributions\n",
        "-------------------------------------------\n",
        "\n",
        "Recall that the unigram distribution is given as:\n",
        "\n",
        "$ P(w_i) = \\frac{  f(w_i)  }{\\sum_{j=0}^{n}\\left(  f(w_j) \\right) } $\n",
        "\n",
        "But the authors found that the following modification produced better results:\n",
        "\n",
        "$ P(w_i) = \\frac{  {f(w_i)}^{3/4}  }{\\sum_{j=0}^{n}\\left(  {f(w_j)}^{3/4} \\right) } $\n",
        "\n",
        "Let's compare the probabilities for the 10 most common and 10 least common words to see how the modified distribution affects them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp2Tiy5NLYBM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "041ac5fb-5cfb-4f2a-b010-ca12b6462de1"
      },
      "source": [
        "# List to hold the data to report.\n",
        "rows = []\n",
        "\n",
        "# Indeces for the first ten and last ten words.\n",
        "indeces = list(range(0, 10))\n",
        "indeces += list(range(len(entries)-10, len(entries)))\n",
        "\n",
        "# For each word...\n",
        "for i in indeces:\n",
        "    # Look up the word.\n",
        "    word = model.wv.index2word[entries[i].index]\n",
        "\n",
        "    # Get the probability of selection with unigram distribution.\n",
        "    # Format it as a percentage with 2 decimal points.\n",
        "    uni_prob = '%.6f%%' % (float(entries[i].count) / float(total_count) * 100.0)\n",
        "\n",
        "    # Get the probability of selection with\n",
        "    uni34_prob = '%.6f%%' % (pow(entries[i].count, power) / train_words_pow * 100.0)\n",
        "\n",
        "    #rows.append((word, entries[i].count, prob_str))\n",
        "    rows.append((i, word, uni_prob, uni34_prob))\n",
        "\n",
        "# Convert the results to a DataFrame to display as a nice table.\n",
        "df = pd.DataFrame(rows, columns=['Rank', 'Word', 'Unigram', 'Unigram^3/4'])\n",
        "df = df.set_index('Rank')\n",
        "\n",
        "display(df)\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Unigram</th>\n",
              "      <th>Unigram^3/4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rank</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>4.759013%</td>\n",
              "      <td>1.301913%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>to</td>\n",
              "      <td>2.846351%</td>\n",
              "      <td>0.885443%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and</td>\n",
              "      <td>2.271361%</td>\n",
              "      <td>0.747582%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>you</td>\n",
              "      <td>2.251390%</td>\n",
              "      <td>0.742647%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>of</td>\n",
              "      <td>2.232659%</td>\n",
              "      <td>0.738008%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>is</td>\n",
              "      <td>1.794802%</td>\n",
              "      <td>0.626552%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>that</td>\n",
              "      <td>1.572681%</td>\n",
              "      <td>0.567446%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>in</td>\n",
              "      <td>1.441577%</td>\n",
              "      <td>0.531585%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>it</td>\n",
              "      <td>1.438820%</td>\n",
              "      <td>0.530822%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>this</td>\n",
              "      <td>0.956311%</td>\n",
              "      <td>0.390745%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71028</th>\n",
              "      <td>kishanchhetri</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71029</th>\n",
              "      <td>djing</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71030</th>\n",
              "      <td>dehradun</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71031</th>\n",
              "      <td>ghor</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71032</th>\n",
              "      <td>shamsuddin</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71033</th>\n",
              "      <td>ghiasuddin</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71034</th>\n",
              "      <td>ruknuddin</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71035</th>\n",
              "      <td>choiceless</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71036</th>\n",
              "      <td>semenuk</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71037</th>\n",
              "      <td>fuggin</td>\n",
              "      <td>0.000026%</td>\n",
              "      <td>0.000149%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Word    Unigram Unigram^3/4\n",
              "Rank                                       \n",
              "0                the  4.759013%   1.301913%\n",
              "1                 to  2.846351%   0.885443%\n",
              "2                and  2.271361%   0.747582%\n",
              "3                you  2.251390%   0.742647%\n",
              "4                 of  2.232659%   0.738008%\n",
              "5                 is  1.794802%   0.626552%\n",
              "6               that  1.572681%   0.567446%\n",
              "7                 in  1.441577%   0.531585%\n",
              "8                 it  1.438820%   0.530822%\n",
              "9               this  0.956311%   0.390745%\n",
              "71028  kishanchhetri  0.000026%   0.000149%\n",
              "71029          djing  0.000026%   0.000149%\n",
              "71030       dehradun  0.000026%   0.000149%\n",
              "71031           ghor  0.000026%   0.000149%\n",
              "71032     shamsuddin  0.000026%   0.000149%\n",
              "71033     ghiasuddin  0.000026%   0.000149%\n",
              "71034      ruknuddin  0.000026%   0.000149%\n",
              "71035     choiceless  0.000026%   0.000149%\n",
              "71036        semenuk  0.000026%   0.000149%\n",
              "71037         fuggin  0.000026%   0.000149%"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3eUWb7FLYBP"
      },
      "source": [
        "### 20 Random Words\n",
        "-------------------------\n",
        "It's interesting to use this table now and select a handful of words at random to see what we get.\n",
        "\n",
        "What we see, most notably, is that common words are still sampled very often!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IoWrDeZLYBP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "16218938-0b1c-4a13-bb48-1013cee896a8"
      },
      "source": [
        "import random\n",
        "\n",
        "print('  --Rank--   --Word--')\n",
        "\n",
        "# Pick some negative samples!\n",
        "for i in range(0, 20):\n",
        "\n",
        "    # Pick a random index in the unigram table.\n",
        "    j = random.randint(0, table_size)\n",
        "\n",
        "    # Look up the word at position 'j'.\n",
        "    word_i = uni_table[j, 0]\n",
        "\n",
        "    # Print the word's ranking  with its ranking.\n",
        "    print(\"{:>10,}   {:}\".format(word_i, model.wv.index2word[int(word_i)]))\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  --Rank--   --Word--\n",
            "        60   here\n",
            "     1,105   error\n",
            "         8   it\n",
            "    19,841   virulent\n",
            "        86   re\n",
            "    22,321   musk\n",
            "         0   the\n",
            "    68,145   darknesshines\n",
            "    32,757   sprawling\n",
            "        14   as\n",
            "     2,282   promise\n",
            "    40,531   spiketoronto\n",
            "     2,034   cellspacing\n",
            "       366   important\n",
            "       368   states\n",
            "       423   following\n",
            "        49   who\n",
            "    32,269   ded\n",
            "        34   what\n",
            "     5,623   shocked\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AVGjWA3LYBR"
      },
      "source": [
        "## Backprop\n",
        "-----------------\n",
        "Now we're ready to run Backprop!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPEHpx9LYBS"
      },
      "source": [
        "### Retrieve Weight Matrices\n",
        "--------------------------------\n",
        "\n",
        "We're going to use the weight matrices from our partially trained gensim model.\n",
        "\n",
        "The input vectors are stored in `model.wv.vectors`, and the output vectors are stored in `model.trainables.syn1neg`.\n",
        "\n",
        "We'll run some sanity checks here to ensure that the vectors aren't normalized and that we are indexing them correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh898ZQXLYBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "4d865c02-a901-4123-e49a-409bc414f15c"
      },
      "source": [
        "# Get the (partially-trained) input vectors matrix and the\n",
        "# output vectors matrix from the gensim model.\n",
        "in_vecs = model.wv.vectors\n",
        "out_vecs = model.trainables.syn1neg\n",
        "\n",
        "# Retrieve the word vector for \"stupid\".\n",
        "vec_a_i = model.wv.vocab['stupid'].index\n",
        "vec_a = in_vecs[vec_a_i,:]\n",
        "\n",
        "# Retrieve the word vector for \"dumb\".\n",
        "vec_b_i = model.wv.vocab['dumb'].index\n",
        "vec_b = in_vecs[vec_b_i,:]\n",
        "\n",
        "# Ensure that the vectors aren't normalized!\n",
        "print('Verifying vectors are not normalized...')\n",
        "assert((np.linalg.norm(vec_a) - 1.0) > 0.01)\n",
        "assert((np.linalg.norm(vec_b) - 1.0) > 0.01)\n",
        "\n",
        "print(\"Cosine similarity between 'stupid' and 'dumb'...\")\n",
        "print(\"    Using gensim: %.4f\" % model.wv.similarity('stupid', 'dumb'))\n",
        "print(\"        Manually: %.4f\" % np.dot(vec_a / np.linalg.norm(vec_a), vec_b / np.linalg.norm(vec_b)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Verifying vectors are not normalized...\n",
            "Cosine similarity between 'stupid' and 'dumb'...\n",
            "    Using gensim: 0.8387\n",
            "        Manually: 0.8387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvQYvsc8LYBV"
      },
      "source": [
        "### Picking Samples\n",
        "-----------------------\n",
        "\n",
        "Here is an example sentence from the training data: \"your questions are well thought out and reasoned\"\n",
        "\n",
        "Let's suppose that our context window is currently centered around `thought`, and we are currently looking at the word at -1, `well`. This is our positive sample.\n",
        "\n",
        "```\n",
        "            -3       -2     -1      input     +1     +2      +3      \n",
        "\"your  (questions)  (are)  (well)  [thought]  (out)  (and) (reasoned)\"\n",
        "```\n",
        "\n",
        "We'll start by:\n",
        "1. Defining our input word ('thought').\n",
        "2. Defining our positive output word ('well').\n",
        "3. Selecting five random words as negative samples.\n",
        "\n",
        "We'll store the output words as a list with entries of the form (`word`, `label`), where `label` is 1 for the positive sample and 0 for the negative samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPFDng06LYBV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "b216b3d3-2f23-4001-de89-bd76528f598b"
      },
      "source": [
        "import random\n",
        "\n",
        "# The word at the center of our context window.\n",
        "input_word = 'thought'\n",
        "\n",
        "# Build a table to report the words--this table is just for information\n",
        "# and won't be used in the training.\n",
        "word_stats = []\n",
        "\n",
        "# Create a list of positive and negative output words with their labels.\n",
        "output_words = [('well', 1.0)]\n",
        "\n",
        "# Record the word, its label, its frequency rank, and the number of occurrences in the training data.\n",
        "word_stats.append(('well',\n",
        "                   1.0,  # Label\n",
        "                   \"{:,}\".format(model.wv.vocab['well'].index), # Frequency rank\n",
        "                   \"{:,}\".format(model.wv.vocab['well'].count) # Number of occurrences\n",
        "                  ))\n",
        "\n",
        "# The number of negative samples is a parameter--the default is 5.\n",
        "num_neg_samples = 5\n",
        "\n",
        "# Randomly choose 5 negative samples.\n",
        "for i in range(0, num_neg_samples):\n",
        "\n",
        "    # Pick a random index in the unigram table.\n",
        "    j = random.randint(0, table_size)\n",
        "\n",
        "    # Look up the word at position 'j'.\n",
        "    word_i = uni_table[j, 0]\n",
        "\n",
        "    # Retrieve the string version of the word.\n",
        "    out_word = model.wv.index2word[int(word_i)]\n",
        "\n",
        "    # Record the word, its label, its frequency rank, and the number of occurrences in the training data.\n",
        "    word_stats.append((out_word,\n",
        "                       0.0, # Label\n",
        "                       \"{:,}\".format(word_i),  # Frequency rank\n",
        "                       \"{:,}\".format(model.wv.vocab[out_word].count) # Number of occurrences\n",
        "                      ))\n",
        "\n",
        "    # Add the word to the list, with label '0' to indicate it's a negative\n",
        "    # sample.\n",
        "    output_words.append((out_word, 0.0))\n",
        "\n",
        "# Display our output words and their statistics as a table.\n",
        "df = pd.DataFrame(word_stats, columns=['Word', 'Label', 'Rank', 'Occurrences'])\n",
        "df = df.set_index('Word')\n",
        "display(df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Rank</th>\n",
              "      <th>Occurrences</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Word</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>well</th>\n",
              "      <td>1.0</td>\n",
              "      <td>94</td>\n",
              "      <td>9,342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jihads</th>\n",
              "      <td>0.0</td>\n",
              "      <td>43,314</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>steamboat</th>\n",
              "      <td>0.0</td>\n",
              "      <td>27,608</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>you</th>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>170,678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gfdl</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2,846</td>\n",
              "      <td>227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>215,782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Label    Rank Occurrences\n",
              "Word                                \n",
              "well         1.0      94       9,342\n",
              "jihads       0.0  43,314           3\n",
              "steamboat    0.0  27,608           7\n",
              "you          0.0       3     170,678\n",
              "gfdl         0.0   2,846         227\n",
              "to           0.0       1     215,782"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVdJnC_7LYBX"
      },
      "source": [
        "### Weight Updates\n",
        "----------------------\n",
        "\n",
        "We now have 6 word pairs to train the network on, one positive pair and five negative pairs.\n",
        "\n",
        "For each output word, we will:\n",
        "\n",
        "1. Calculate the network's current output for this output word.\n",
        "2. Calculate the error.\n",
        "3. Update the output weights for the output word.\n",
        "4. Accumulate the gradient for the input word.\n",
        "\n",
        "The term \"gradient\" just means the amount to update each weight by. Specifically, we'll have a gradient vector which is the same size as the input word vector, and it will store the amount to adjust each feature of the input word vector by."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ninwHjLYBX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "47eece4d-4e7a-4380-a2e6-d8134715f6bc"
      },
      "source": [
        "# Learning rate. This starts out at this value, but changes\n",
        "# over the course of training.\n",
        "alpha = 0.025\n",
        "\n",
        "# For reference...\n",
        "# index 2 word: model.wv.index2word\n",
        "# word 2 index: model.wv.vocab\n",
        "\n",
        "# Record the activation and error values to display in a table at\n",
        "# the end.\n",
        "out_word_stats = []\n",
        "\n",
        "# Look up the index for the input word.\n",
        "in_vec_i = model.wv.vocab[input_word].index\n",
        "\n",
        "# Select the input word vector.\n",
        "in_vec = in_vecs[in_vec_i, :]\n",
        "\n",
        "# Create an empty vector to hold the gradient for the input word.\n",
        "in_vec_grad = np.zeros(in_vecs[0,:].shape)\n",
        "\n",
        "# Print header\n",
        "#print('   Input         Output   Activ.  Error')\n",
        "#print('   -----         ------   ------  -----')\n",
        "\n",
        "# For each output word...\n",
        "for (out_word, label) in output_words:\n",
        "\n",
        "    # ======== Calculate Network Output ========\n",
        "\n",
        "    # Look up the output word.\n",
        "    out_vec_i = model.wv.vocab[out_word].index\n",
        "\n",
        "    # Select the output word vector.\n",
        "    out_vec = out_vecs[out_vec_i, :]\n",
        "\n",
        "    # Take their dot product.\n",
        "    z = np.dot(in_vec, out_vec)\n",
        "\n",
        "    # Apply the sigmoid activation. This is the model's output for\n",
        "    # `out_word`\n",
        "    activation = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # ======== Calculate Error ========\n",
        "\n",
        "    # Calculate the output error and apply the learning rate (alpha).\n",
        "    err = (label - activation) * alpha\n",
        "\n",
        "    # ======== Update Output Word Vector ========\n",
        "\n",
        "    # Update the output vector by multiplying the error with the input\n",
        "    # vector.\n",
        "    out_vecs[out_vec_i, :] = out_vec + (err * in_vec)\n",
        "\n",
        "    # ======== Accumulate Input Word Vector Gradient ========\n",
        "\n",
        "    # Multiply the error with the output vector and accumulate this\n",
        "    # as the gradient for the input vector.\n",
        "    in_vec_grad += err * out_vec\n",
        "\n",
        "    # Record the activation and error for this word pair.\n",
        "    # Leave the last column empty--it will hold the new activation value\n",
        "    # in the next code block.\n",
        "    out_word_stats.append([input_word, out_word, '%.4f' % activation, '%.4f' % err, ''])\n",
        "\n",
        "    #print('%10s  %12s  %.4f  %.4f' % (input_word, out_word, activation, err))\n",
        "\n",
        "# ======== Apply Input Word Vector Gradient ========\n",
        "\n",
        "# Update the input word vector weights by applying the gradient.\n",
        "in_vecs[in_vec_i, :] = in_vec + in_vec_grad\n",
        "\n",
        "# Display the activation and error values as a table.\n",
        "df = pd.DataFrame(out_word_stats,\n",
        "                  columns=['Input', 'Output', 'Activation', 'Error', ''])\n",
        "\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Output</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Error</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thought</td>\n",
              "      <td>well</td>\n",
              "      <td>0.8085</td>\n",
              "      <td>0.0048</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thought</td>\n",
              "      <td>jihads</td>\n",
              "      <td>0.1248</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought</td>\n",
              "      <td>steamboat</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>-0.0017</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>thought</td>\n",
              "      <td>you</td>\n",
              "      <td>0.7931</td>\n",
              "      <td>-0.0198</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thought</td>\n",
              "      <td>gfdl</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>thought</td>\n",
              "      <td>to</td>\n",
              "      <td>0.0852</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Input     Output Activation    Error  \n",
              "0  thought       well     0.8085   0.0048  \n",
              "1  thought     jihads     0.1248  -0.0031  \n",
              "2  thought  steamboat     0.0660  -0.0017  \n",
              "3  thought        you     0.7931  -0.0198  \n",
              "4  thought       gfdl     0.0023  -0.0001  \n",
              "5  thought         to     0.0852  -0.0021  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HFjDDrALYBa"
      },
      "source": [
        "#### Side Note - Interpreting the Activation\n",
        "-----------------------------------------\n",
        "Don't get confused about the meaning of the activation value--*it is not a measure of word similarity*. Rather, it reflects how likely you are to find the word \"well\" in the vicinity of \"thought\".\n",
        "\n",
        "For example, the words \"thought\" and \"think\" are very similar in meaning, but are unlikely to appear close together. The following code confirms this with our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApwD-hfCLYBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "108523ef-0679-42c4-8ba6-aacbd224caa3"
      },
      "source": [
        "# Select output vector for \"think\".\n",
        "out_vec = out_vecs[model.wv.vocab['think'].index, :]\n",
        "\n",
        "# Take dot product of \"thought\" and \"think\".\n",
        "z = np.dot(in_vec, out_vec)\n",
        "\n",
        "# Apply the sigmoid activation.\n",
        "activation = 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Show the word vector similarity versus output value.\n",
        "print(\"Similarity for 'thought' and 'think':    %.4f\" % model.wv.similarity('thought', 'think'))\n",
        "print(\"Network output for ('thought', 'think'): %.4f\" % activation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity for 'thought' and 'think':    0.4503\n",
            "Network output for ('thought', 'think'): 0.1073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGDmPKT5LYBc"
      },
      "source": [
        "------------------------\n",
        "Now that we've updated the weights, we can try running another forward pass to see the impact of our changes.\n",
        "\n",
        "We'll see that the positive output word now has a slightly higher activation, and the negative output words now all have slightly lower activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1RXCFw2LYBc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "5f8c7de4-76a5-4083-f54b-5dda772e0fa9"
      },
      "source": [
        "# Select the updated input word vector.\n",
        "in_vec = in_vecs[in_vec_i, :]\n",
        "\n",
        "i = 0\n",
        "\n",
        "# For each output word...\n",
        "for (out_word, label) in output_words:\n",
        "\n",
        "    # Look up the output word.\n",
        "    out_vec_i = model.wv.vocab[out_word].index\n",
        "\n",
        "    # Select the updated output word vector.\n",
        "    out_vec = out_vecs[out_vec_i, :]\n",
        "\n",
        "    # Take their dot product.\n",
        "    z = np.dot(in_vec, out_vec)\n",
        "\n",
        "    # Apply the sigmoid activation. This is the model's output for\n",
        "    # `out_word`\n",
        "    activation = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # Record the new activation value\n",
        "    out_word_stats[i][4] = '%.4f' % activation\n",
        "\n",
        "    i += 1\n",
        "\n",
        "df = pd.DataFrame(out_word_stats, columns=['Input', 'Output', 'Prev. Activation', 'Prev. Error', 'New Activation'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Output</th>\n",
              "      <th>Prev. Activation</th>\n",
              "      <th>Prev. Error</th>\n",
              "      <th>New Activation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thought</td>\n",
              "      <td>well</td>\n",
              "      <td>0.8085</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.8831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thought</td>\n",
              "      <td>jihads</td>\n",
              "      <td>0.1248</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought</td>\n",
              "      <td>steamboat</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>-0.0017</td>\n",
              "      <td>0.0535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>thought</td>\n",
              "      <td>you</td>\n",
              "      <td>0.7931</td>\n",
              "      <td>-0.0198</td>\n",
              "      <td>0.2406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thought</td>\n",
              "      <td>gfdl</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td>0.0022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>thought</td>\n",
              "      <td>to</td>\n",
              "      <td>0.0852</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>0.0646</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Input     Output Prev. Activation Prev. Error New Activation\n",
              "0  thought       well           0.8085      0.0048         0.8831\n",
              "1  thought     jihads           0.1248     -0.0031         0.0875\n",
              "2  thought  steamboat           0.0660     -0.0017         0.0535\n",
              "3  thought        you           0.7931     -0.0198         0.2406\n",
              "4  thought       gfdl           0.0023     -0.0001         0.0022\n",
              "5  thought         to           0.0852     -0.0021         0.0646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8mnvwLGLYBe"
      },
      "source": [
        "# Conclusion\n",
        "---------------------\n",
        "\n",
        "Here's what we covered:\n",
        "* We saw how negative sampling is implemented using the unigram table approach.\n",
        "* We looked more at how negative sampling behaves on real data.\n",
        "* We saw how to update the skip-gram model weights by walking through a single training sample."
      ]
    }
  ]
}