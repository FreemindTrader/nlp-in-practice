{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K5Bmp33iQJHi"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FreemindTrader/nlp-in-practice/blob/master/Chapter_6_fastText_Training_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWS0dRpfMVPd"
      },
      "source": [
        "# Introduction\n",
        "-----------------------\n",
        "\n",
        "This notebook will demonstrate training a word2vec model **with subword information** (fastText) on the Wikipedia Attack Comments dataset. We'll look at how the training time and memory requirements compare, as well as the quality of the resulting vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5Bmp33iQJHi"
      },
      "source": [
        "# Download & Parse Dataset\n",
        "------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVSLZLgNOqHo"
      },
      "source": [
        "We'll use:\n",
        "\n",
        "* `wget` to download the dataset file.\n",
        "* `pandas` to parse the dataset `.tsv` file.\n",
        "* The `gensim` function `gensim.utils.simple_preprocess` for tokenizing the sentences.\n",
        "\n",
        "We'll need to install wget first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5012GKsOsYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "86e2f6bb-62be-44df-fac2-5c97d2643801"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=767b5d2835f689eb9318018aa4aedc3769d70eebc5740222016a333c89c505af\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av71jxKnRU5b"
      },
      "source": [
        "Now we can download the dataset text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeXDiQeh3CSz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c131c014-aeb9-491f-8c8d-35d15667978d"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "# Create the data subdirectory if not there.\n",
        "if not os.path.exists('./data/'):\n",
        "    os.mkdir('./data/')\n",
        "\n",
        "filename = './data/attack_annotated_comments.tsv'\n",
        "\n",
        "# Download download if we already have it!\n",
        "if not os.path.exists(filename):\n",
        "\n",
        "    # URL for the CSV file (~55.4MB) containing the wikipedia comments.\n",
        "    url = 'https://ndownloader.figshare.com/files/7554634'\n",
        "\n",
        "    # Download the dataset.\n",
        "    print('Downloading Wikipedia Attack Comments dataset (~55.4MB)...')\n",
        "    wget.download(url, filename)\n",
        "\n",
        "    print('  DONE.')\n",
        "\n",
        "# We won't use these, but FYI, this is the file containing the labels\n",
        "# for the comments.\n",
        "#   url = 'https://ndownloader.figshare.com/files/7554637'\n",
        "#   filename = './data/attack_annotated_comments.tsv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Wikipedia Attack Comments dataset (~55.4MB)...\n",
            "  DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyVtvfjy3CS6"
      },
      "source": [
        "## Parse the dataset file\n",
        "--------------------------------\n",
        "We'll use `pandas` just to help us parse the tab-separated `.tsv` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wct9ctus3CS7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4471ce16-5f37-44ff-f595-de3dffdd9908"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "print('Parsing the dataset .tsv file...')\n",
        "comments = pd.read_csv('./data/attack_annotated_comments.tsv', sep = '\\t')\n",
        "\n",
        "print('    Done.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing the dataset .tsv file...\n",
            "    Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSQHqEeI3CTC"
      },
      "source": [
        "## Tokenize the comments\n",
        "------------------------------------\n",
        "This dataset uses the special labels \"NEWLINE_TOKEN\" and \"TAB_TOKEN\" to represent the newline and tab characters. We'll replace these with a single space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bB9AntK3CTD"
      },
      "source": [
        "# remove newline and tab tokens\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7ZWw_Qi3CTI"
      },
      "source": [
        "Next, Use gensim to perform a simple tokenization strategy to the text and turn each comment into a list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB9v4PNQ3CTJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "cebfcd31-fc33-434e-dd28-a73b30d3b07f"
      },
      "source": [
        "%%time\n",
        "\n",
        "import gensim\n",
        "import io\n",
        "\n",
        "print('Tokenizing comments...')\n",
        "\n",
        "# Track the total number of tokens in the dataset.\n",
        "num_tokens = 0\n",
        "\n",
        "# List of sentences to use for training.\n",
        "sentences = []\n",
        "\n",
        "# For each comment...\n",
        "for i, row in comments.iterrows():\n",
        "\n",
        "    # Report progress.\n",
        "    if ((i % 20000) == 0):\n",
        "        print('  Read {:,} comments.'.format(i))\n",
        "\n",
        "    # Tokenize the comment. This returns a list of words.\n",
        "    parsed = gensim.utils.simple_preprocess(row.comment)\n",
        "\n",
        "    # Accumulate the total number of words in the dataset.\n",
        "    num_tokens += len(parsed)\n",
        "\n",
        "    # Add the comment to the list.\n",
        "    sentences.append(parsed)\n",
        "\n",
        "print('DONE.')\n",
        "print('')\n",
        "print('{:>10,} comments'.format(i))\n",
        "print('{:>10,} tokens'.format(num_tokens))\n",
        "print('{:>10,} avg. tokens / comment'.format(int(num_tokens / len(sentences))))\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing comments...\n",
            "  Read 0 comments.\n",
            "  Read 20,000 comments.\n",
            "  Read 40,000 comments.\n",
            "  Read 60,000 comments.\n",
            "  Read 80,000 comments.\n",
            "  Read 100,000 comments.\n",
            "DONE.\n",
            "\n",
            "   115,863 comments\n",
            " 7,651,029 tokens\n",
            "        66 avg. tokens / comment\n",
            "\n",
            "CPU times: user 22.9 s, sys: 516 ms, total: 23.5 s\n",
            "Wall time: 23.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcTlAgShMVPg"
      },
      "source": [
        "# Training\n",
        "----------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViPxkz6sRc60"
      },
      "source": [
        "Time to train the model!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ83ezKPMVPq"
      },
      "source": [
        "## Configure Logging\n",
        "-----------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaPKMo7vZ7Wj"
      },
      "source": [
        "\n",
        "`gensim` provides some valuable information about the training process using the `logging` module in Python.\n",
        "\n",
        "In order to see this log output, we first need to setup logging.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCZbjI55MVPr"
      },
      "source": [
        "import logging\n",
        "\n",
        "# Enable logging at the `INFO` level and set a custom format--the\n",
        "# default log format is pretty wordy.\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s : %(message)s', # Display just time and message.\n",
        "    datefmt='%H:%M:%S', # Display time, but not the date.\n",
        "    level=logging.INFO)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeivjqk9b7UH"
      },
      "source": [
        "The following settings will eliminate some unhelpful warnings from the remainder of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZiG46rdb2tg"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpDQPXhUMVPx"
      },
      "source": [
        "## Set Model Parameters\n",
        "----------------------------------\n",
        "\n",
        "We define all of the parameters for our model upfront. Take a look at the code comments for each parameter below.\n",
        "\n",
        "Also, for reference, here is the [documentation](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.trainables) for the `gensim.models.FastText` constructor, and the [source code](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/fasttext.py#L468) on GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6sNlps-MVPy"
      },
      "source": [
        "import gensim\n",
        "\n",
        "model = gensim.models.FastText (\n",
        "    sentences=None, # Don't provide the sentences yet, otherwise\n",
        "                    # it will kick off the training automatically.\n",
        "\n",
        "    size=100,    # Number of features in word vector\n",
        "\n",
        "    window=10,   # Context window size (in each direction)\n",
        "                 #   Default is 5\n",
        "\n",
        "    min_count=2, # Words must appear this many times to be in vocab.\n",
        "                 #   Default is 5\n",
        "\n",
        "    workers=10,  # Training thread count\n",
        "\n",
        "    sg=0,        # 0: CBOW, 1: Skip-gram.\n",
        "                 #   Default is 0, CBOW\n",
        "\n",
        "    hs=0,        # 0: Negative Sampling, 1: Hierarchical Softmax\n",
        "                 #   Default is 0, NS\n",
        "\n",
        "    negative=5,  # Nmber of negative samples (default is 5)\n",
        "\n",
        "    sample=1e-3, # The coefficient for the subsampling of frequent words\n",
        "                 # equation.\n",
        "\n",
        "    word_ngrams=1, # Turn on n-grams.\n",
        "    min_n=3,       # Min n-gram size of 3 characters (default is 3).\n",
        "    max_n=6,       # Max n-gram size of 6 characters (default is 6).\n",
        "\n",
        "    bucket=2000000, # Initial number of buckets for the n-gram hash table.\n",
        "                    # gensim appears to resize the hash table for you, though,\n",
        "                    # as part of building the vocabulary.\n",
        "\n",
        "    # Additional parameters and their defaults...\n",
        "\n",
        "    # seed=1,\n",
        "    # alpha=0.025,    # Initial learning rate.\n",
        "    # min_alpha=0.0001,\n",
        "    # cbow_mean=1,\n",
        "    # hashfxn=hash,\n",
        "    # null_word=0,\n",
        "    # sorted_vocab=1,\n",
        "    # trim_rule=None,\n",
        "    # batch_words=MAX_WORDS_IN_BATCH,\n",
        "    # callbacks=()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd3T7Y6LMVP6"
      },
      "source": [
        "## Build the Vocabulary\n",
        "---------------------------------\n",
        "\n",
        "Before we can train the word2vec neural network, we need to create a vocabulary. The vocabulary contains the full list of words that we will end up learning word vectors for.\n",
        "\n",
        "* Source code for `build_vocab` is at [base_any2vec.py#L896](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/base_any2vec.py#L896)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9JOKirMVP7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "7a0865c4-1f15-4fb4-c039-83668345a7bf"
      },
      "source": [
        "# Build the vocabulary using the comments in \"sentences\".\n",
        "model.build_vocab(\n",
        "    sentences, # Our comments dataset\n",
        "    progress_per=20000  # Update after this many sentences.\n",
        "                        # Too many progress updates is annoying!\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22:41:13 : collecting all words and their counts\n",
            "22:41:13 : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "22:41:13 : PROGRESS: at sentence #20000, processed 1399843 words, keeping 51238 word types\n",
            "22:41:14 : PROGRESS: at sentence #40000, processed 2764033 words, keeping 76280 word types\n",
            "22:41:14 : PROGRESS: at sentence #60000, processed 4091968 words, keeping 94720 word types\n",
            "22:41:14 : PROGRESS: at sentence #80000, processed 5354741 words, keeping 112164 word types\n",
            "22:41:15 : PROGRESS: at sentence #100000, processed 6640746 words, keeping 129161 word types\n",
            "22:41:15 : collected 141062 word types from a corpus of 7651029 raw words and 115864 sentences\n",
            "22:41:15 : Loading a fresh vocabulary\n",
            "22:41:15 : effective_min_count=2 retains 71038 unique words (50% of original 141062, drops 70024)\n",
            "22:41:15 : effective_min_count=2 leaves 7581005 word corpus (99% of original 7651029, drops 70024)\n",
            "22:41:16 : deleting the raw counts dictionary of 141062 items\n",
            "22:41:16 : sample=0.001 downsamples 48 most-common words\n",
            "22:41:16 : downsampling leaves estimated 5905950 word corpus (77.9% of prior 7581005)\n",
            "22:41:17 : estimated required memory for 71038 words, 336317 buckets and 100 dimensions: 243616904 bytes\n",
            "22:41:17 : resetting layer weights\n",
            "22:41:33 : Total number of ngrams is 336317\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn7EzrkqMVQC"
      },
      "source": [
        "-------------------------------\n",
        "The logging output above displays the estimated size of the model as `243616904 bytes` (232.3 MB). From the same logging output of the word2vec model, that one is estimated at 88.1 MB, so fastText requires 2.64x more memory.\n",
        "\n",
        "TBD - This does not correspond directly to the expected matrix size, so perhaps it includes the memory for the vocabulary as well?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuFtCH8dMVQD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ebd4f727-849b-47d8-c22f-8e89a3c1bfa3"
      },
      "source": [
        "print('word2vec model is %.1f MB' % (92349400 / 2**20))\n",
        "print('fastText model is %.1f MB' % (243616904 / 2**20))\n",
        "print()\n",
        "print('fastText model is %.2fx larger' % (243616904 / 92349400))\n",
        "print()\n",
        "print('Expected word2vec matrix size: %.2f MB' % (71038*100*4 / 2**20))\n",
        "print('Expected fastText matrix size: %.2f MB' % ((71038 + 336317)*100*4 / 2**20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word2vec model is 88.1 MB\n",
            "fastText model is 232.3 MB\n",
            "\n",
            "fastText model is 2.64x larger\n",
            "\n",
            "Expected word2vec matrix size: 27.10 MB\n",
            "Expected fastText matrix size: 155.39 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCFHNCJ8MVQI"
      },
      "source": [
        "## Train the model\n",
        "-------------------------\n",
        "\n",
        "Now that we have a vocabulary built, we're ready to train the model.\n",
        "\n",
        "The word2vec model took `43s`  to train on my desktop, while this fastText one took `268s` (4min 28s), which is about `6.2x` longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82wU8o0mMVQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e912a7c6-5d9a-4d3a-cc4b-ce783e460326"
      },
      "source": [
        "%%time\n",
        "\n",
        "print('Training the model...')\n",
        "\n",
        "model.train(\n",
        "    sentences,\n",
        "    total_examples=len(sentences),\n",
        "    epochs=10,        # How many training passes to take.\n",
        "    report_delay=10.0 # Report progress every 10 seconds.\n",
        ")\n",
        "\n",
        "print('  Done.')\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22:41:36 : training model with 10 workers on 71038 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training the model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "22:41:37 : EPOCH 1 - PROGRESS: at 1.28% examples, 64976 words/s, in_qsize 19, out_qsize 0\n",
            "22:41:47 : EPOCH 1 - PROGRESS: at 16.91% examples, 93243 words/s, in_qsize 19, out_qsize 0\n",
            "22:41:57 : EPOCH 1 - PROGRESS: at 33.10% examples, 94474 words/s, in_qsize 19, out_qsize 0\n",
            "22:42:07 : EPOCH 1 - PROGRESS: at 49.46% examples, 95526 words/s, in_qsize 19, out_qsize 0\n",
            "22:42:17 : EPOCH 1 - PROGRESS: at 66.90% examples, 96011 words/s, in_qsize 20, out_qsize 2\n",
            "22:42:27 : EPOCH 1 - PROGRESS: at 83.84% examples, 96154 words/s, in_qsize 20, out_qsize 2\n",
            "22:42:36 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:42:36 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:42:37 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:42:37 : EPOCH - 1 : training on 7651029 raw words (5905194 effective words) took 61.2s, 96548 effective words/s\n",
            "22:42:38 : EPOCH 2 - PROGRESS: at 0.53% examples, 35734 words/s, in_qsize 20, out_qsize 5\n",
            "22:42:48 : EPOCH 2 - PROGRESS: at 16.80% examples, 92930 words/s, in_qsize 19, out_qsize 0\n",
            "22:42:58 : EPOCH 2 - PROGRESS: at 33.02% examples, 95632 words/s, in_qsize 19, out_qsize 0\n",
            "22:43:08 : EPOCH 2 - PROGRESS: at 49.25% examples, 96052 words/s, in_qsize 19, out_qsize 0\n",
            "22:43:18 : EPOCH 2 - PROGRESS: at 66.25% examples, 95808 words/s, in_qsize 19, out_qsize 0\n",
            "22:43:28 : EPOCH 2 - PROGRESS: at 83.12% examples, 95775 words/s, in_qsize 19, out_qsize 3\n",
            "22:43:38 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:43:38 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:43:38 : EPOCH - 2 : training on 7651029 raw words (5906652 effective words) took 61.1s, 96644 effective words/s\n",
            "22:43:39 : EPOCH 3 - PROGRESS: at 1.10% examples, 74449 words/s, in_qsize 20, out_qsize 0\n",
            "22:43:49 : EPOCH 3 - PROGRESS: at 16.53% examples, 93743 words/s, in_qsize 19, out_qsize 0\n",
            "22:43:59 : EPOCH 3 - PROGRESS: at 32.00% examples, 92910 words/s, in_qsize 18, out_qsize 1\n",
            "22:44:09 : EPOCH 3 - PROGRESS: at 48.15% examples, 94414 words/s, in_qsize 20, out_qsize 0\n",
            "22:44:19 : EPOCH 3 - PROGRESS: at 64.93% examples, 94676 words/s, in_qsize 19, out_qsize 0\n",
            "22:44:29 : EPOCH 3 - PROGRESS: at 81.31% examples, 94313 words/s, in_qsize 18, out_qsize 1\n",
            "22:44:40 : EPOCH 3 - PROGRESS: at 98.35% examples, 94285 words/s, in_qsize 12, out_qsize 1\n",
            "22:44:40 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:44:40 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:44:40 : EPOCH - 3 : training on 7651029 raw words (5905244 effective words) took 62.0s, 95170 effective words/s\n",
            "22:44:41 : EPOCH 4 - PROGRESS: at 0.89% examples, 60218 words/s, in_qsize 19, out_qsize 1\n",
            "22:44:51 : EPOCH 4 - PROGRESS: at 15.69% examples, 89353 words/s, in_qsize 19, out_qsize 0\n",
            "22:45:01 : EPOCH 4 - PROGRESS: at 32.00% examples, 93037 words/s, in_qsize 19, out_qsize 0\n",
            "22:45:11 : EPOCH 4 - PROGRESS: at 48.02% examples, 94342 words/s, in_qsize 18, out_qsize 1\n",
            "22:45:21 : EPOCH 4 - PROGRESS: at 65.08% examples, 94743 words/s, in_qsize 19, out_qsize 0\n",
            "22:45:31 : EPOCH 4 - PROGRESS: at 81.82% examples, 94806 words/s, in_qsize 17, out_qsize 2\n",
            "22:45:41 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:45:41 : EPOCH 4 - PROGRESS: at 98.93% examples, 95210 words/s, in_qsize 8, out_qsize 1\n",
            "22:45:41 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:45:41 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:45:41 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:45:42 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:45:42 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:45:42 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:45:42 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:45:42 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:45:42 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:45:42 : EPOCH - 4 : training on 7651029 raw words (5904747 effective words) took 61.7s, 95635 effective words/s\n",
            "22:45:43 : EPOCH 5 - PROGRESS: at 1.26% examples, 79426 words/s, in_qsize 20, out_qsize 0\n",
            "22:45:53 : EPOCH 5 - PROGRESS: at 16.75% examples, 93759 words/s, in_qsize 19, out_qsize 0\n",
            "22:46:03 : EPOCH 5 - PROGRESS: at 32.91% examples, 95523 words/s, in_qsize 19, out_qsize 0\n",
            "22:46:13 : EPOCH 5 - PROGRESS: at 48.93% examples, 95444 words/s, in_qsize 19, out_qsize 0\n",
            "22:46:23 : EPOCH 5 - PROGRESS: at 65.23% examples, 94929 words/s, in_qsize 17, out_qsize 2\n",
            "22:46:33 : EPOCH 5 - PROGRESS: at 82.27% examples, 95262 words/s, in_qsize 19, out_qsize 0\n",
            "22:46:43 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:46:43 : EPOCH 5 - PROGRESS: at 99.11% examples, 95354 words/s, in_qsize 7, out_qsize 0\n",
            "22:46:43 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:46:43 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:46:43 : EPOCH - 5 : training on 7651029 raw words (5905684 effective words) took 61.7s, 95756 effective words/s\n",
            "22:46:44 : EPOCH 6 - PROGRESS: at 0.78% examples, 51863 words/s, in_qsize 19, out_qsize 1\n",
            "22:46:54 : EPOCH 6 - PROGRESS: at 16.30% examples, 92255 words/s, in_qsize 19, out_qsize 0\n",
            "22:47:05 : EPOCH 6 - PROGRESS: at 31.64% examples, 92140 words/s, in_qsize 19, out_qsize 2\n",
            "22:47:15 : EPOCH 6 - PROGRESS: at 47.54% examples, 93066 words/s, in_qsize 19, out_qsize 0\n",
            "22:47:25 : EPOCH 6 - PROGRESS: at 64.40% examples, 94047 words/s, in_qsize 18, out_qsize 1\n",
            "22:47:35 : EPOCH 6 - PROGRESS: at 81.03% examples, 94056 words/s, in_qsize 19, out_qsize 0\n",
            "22:47:45 : EPOCH 6 - PROGRESS: at 97.51% examples, 93993 words/s, in_qsize 19, out_qsize 0\n",
            "22:47:45 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:47:46 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:47:46 : EPOCH - 6 : training on 7651029 raw words (5906562 effective words) took 62.6s, 94384 effective words/s\n",
            "22:47:47 : EPOCH 7 - PROGRESS: at 1.26% examples, 68025 words/s, in_qsize 20, out_qsize 0\n",
            "22:47:57 : EPOCH 7 - PROGRESS: at 16.65% examples, 92006 words/s, in_qsize 19, out_qsize 0\n",
            "22:48:07 : EPOCH 7 - PROGRESS: at 32.45% examples, 93469 words/s, in_qsize 19, out_qsize 0\n",
            "22:48:18 : EPOCH 7 - PROGRESS: at 48.15% examples, 93468 words/s, in_qsize 19, out_qsize 0\n",
            "22:48:28 : EPOCH 7 - PROGRESS: at 65.08% examples, 93484 words/s, in_qsize 19, out_qsize 0\n",
            "22:48:38 : EPOCH 7 - PROGRESS: at 81.85% examples, 93761 words/s, in_qsize 19, out_qsize 0\n",
            "22:48:48 : EPOCH 7 - PROGRESS: at 98.22% examples, 93447 words/s, in_qsize 14, out_qsize 2\n",
            "22:48:48 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:48:48 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:48:48 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:48:48 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:48:48 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:48:48 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:48:48 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:48:48 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:48:49 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:48:49 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:48:49 : EPOCH - 7 : training on 7651029 raw words (5906196 effective words) took 62.6s, 94375 effective words/s\n",
            "22:48:50 : EPOCH 8 - PROGRESS: at 1.10% examples, 73875 words/s, in_qsize 20, out_qsize 0\n",
            "22:49:00 : EPOCH 8 - PROGRESS: at 16.30% examples, 92329 words/s, in_qsize 19, out_qsize 0\n",
            "22:49:10 : EPOCH 8 - PROGRESS: at 32.00% examples, 92933 words/s, in_qsize 19, out_qsize 0\n",
            "22:49:20 : EPOCH 8 - PROGRESS: at 47.83% examples, 93563 words/s, in_qsize 19, out_qsize 1\n",
            "22:49:30 : EPOCH 8 - PROGRESS: at 64.67% examples, 94013 words/s, in_qsize 19, out_qsize 0\n",
            "22:49:40 : EPOCH 8 - PROGRESS: at 81.69% examples, 94466 words/s, in_qsize 19, out_qsize 0\n",
            "22:49:50 : EPOCH 8 - PROGRESS: at 98.07% examples, 94283 words/s, in_qsize 15, out_qsize 0\n",
            "22:49:51 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:49:51 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:49:51 : EPOCH - 8 : training on 7651029 raw words (5905527 effective words) took 62.3s, 94864 effective words/s\n",
            "22:49:52 : EPOCH 9 - PROGRESS: at 1.28% examples, 70282 words/s, in_qsize 19, out_qsize 0\n",
            "22:50:02 : EPOCH 9 - PROGRESS: at 16.53% examples, 91230 words/s, in_qsize 19, out_qsize 0\n",
            "22:50:12 : EPOCH 9 - PROGRESS: at 32.33% examples, 93040 words/s, in_qsize 17, out_qsize 2\n",
            "22:50:22 : EPOCH 9 - PROGRESS: at 48.47% examples, 93992 words/s, in_qsize 19, out_qsize 0\n",
            "22:50:32 : EPOCH 9 - PROGRESS: at 65.50% examples, 94689 words/s, in_qsize 19, out_qsize 0\n",
            "22:50:43 : EPOCH 9 - PROGRESS: at 81.84% examples, 93977 words/s, in_qsize 20, out_qsize 1\n",
            "22:50:53 : EPOCH 9 - PROGRESS: at 98.47% examples, 94096 words/s, in_qsize 11, out_qsize 1\n",
            "22:50:53 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:50:53 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:50:53 : EPOCH - 9 : training on 7651029 raw words (5906379 effective words) took 62.3s, 94803 effective words/s\n",
            "22:50:54 : EPOCH 10 - PROGRESS: at 0.85% examples, 58111 words/s, in_qsize 15, out_qsize 2\n",
            "22:51:04 : EPOCH 10 - PROGRESS: at 15.81% examples, 88748 words/s, in_qsize 13, out_qsize 6\n",
            "22:51:14 : EPOCH 10 - PROGRESS: at 31.88% examples, 92118 words/s, in_qsize 20, out_qsize 1\n",
            "22:51:25 : EPOCH 10 - PROGRESS: at 47.93% examples, 93511 words/s, in_qsize 16, out_qsize 3\n",
            "22:51:35 : EPOCH 10 - PROGRESS: at 65.52% examples, 94821 words/s, in_qsize 19, out_qsize 0\n",
            "22:51:45 : EPOCH 10 - PROGRESS: at 82.42% examples, 95066 words/s, in_qsize 20, out_qsize 1\n",
            "22:51:55 : EPOCH 10 - PROGRESS: at 98.47% examples, 94537 words/s, in_qsize 12, out_qsize 0\n",
            "22:51:55 : worker thread finished; awaiting finish of 9 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 8 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 7 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 6 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 5 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 4 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 3 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 2 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 1 more threads\n",
            "22:51:55 : worker thread finished; awaiting finish of 0 more threads\n",
            "22:51:55 : EPOCH - 10 : training on 7651029 raw words (5904573 effective words) took 62.0s, 95186 effective words/s\n",
            "22:51:55 : training on a 76510290 raw words (59056758 effective words) took 619.6s, 95313 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Done.\n",
            "\n",
            "CPU times: user 20min 22s, sys: 2.77 s, total: 20min 25s\n",
            "Wall time: 10min 23s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii2e_KlCMVQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d23f9b31-60bc-47f3-9d19-e88bc5126490"
      },
      "source": [
        "# Write the model out to disk\n",
        "model.save('./data/wiki_attack_ft.model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22:51:59 : saving FastText object under ./data/wiki_attack_ft.model, separately None\n",
            "22:51:59 : storing np array 'vectors_ngrams' to ./data/wiki_attack_ft.model.wv.vectors_ngrams.npy\n",
            "22:51:59 : not storing attribute vectors_norm\n",
            "22:51:59 : not storing attribute vectors_vocab_norm\n",
            "22:51:59 : not storing attribute vectors_ngrams_norm\n",
            "22:51:59 : not storing attribute buckets_word\n",
            "22:51:59 : storing np array 'vectors_ngrams_lockf' to ./data/wiki_attack_ft.model.trainables.vectors_ngrams_lockf.npy\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "22:52:01 : saved ./data/wiki_attack_ft.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJs6XBpcMVQU"
      },
      "source": [
        "## Compare results\n",
        "---------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wznlg1NaZAez"
      },
      "source": [
        "\n",
        "Let's load both the word2vec and fasttext models so that we can compare them side-by-side.\n",
        "\n",
        "I've hosted a copy of the trained word2vec model created by the \"Appendix - Full word2vec Training Example.ipynb\" Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMl5gBz_9bfQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "10dfc1b2-d71e-4e62-dcde-d2cae8e442cc"
      },
      "source": [
        "import gdown\n",
        "\n",
        "print('Downloading word2vec model...\\n')\n",
        "\n",
        "# Specify the name to give the file locally.\n",
        "output = 'wiki_attack_w2v.model'\n",
        "\n",
        "# Specify the Google Drive ID of the file.\n",
        "file_id = '1atZ7L6DqT_zZtIkUnPi77KA7Qsv5zm07'\n",
        "\n",
        "# Download the file.\n",
        "gdown.download('https://drive.google.com/uc?id=' + file_id, output,\n",
        "                quiet=False)\n",
        "\n",
        "print('\\nDONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading word2vec model...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1atZ7L6DqT_zZtIkUnPi77KA7Qsv5zm07\n",
            "To: /content/wiki_attack_w2v.model\n",
            "89.9MB [00:00, 234MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GLUNq_0Zmoz"
      },
      "source": [
        "Now we can load the word2vec model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwjKQPvnMVQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "c802aac2-bba0-4772-b771-a7ea042e200a"
      },
      "source": [
        "import os\n",
        "# Distinguish the fasttext model from the plain word2vec.\n",
        "model_ft = model\n",
        "\n",
        "# Specify the path to the word2vec model that we trained in Chapter 4.\n",
        "w2v_filename = 'wiki_attack_w2v.model'\n",
        "\n",
        "# Load the trained w2v model.\n",
        "model_w2v = gensim.models.Word2Vec.load(w2v_filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23:17:07 : loading Word2Vec object from wiki_attack_w2v.model\n",
            "23:17:08 : loading wv recursively from wiki_attack_w2v.model.wv.* with mmap=None\n",
            "23:17:08 : setting ignored attribute vectors_norm to None\n",
            "23:17:08 : loading vocabulary recursively from wiki_attack_w2v.model.vocabulary.* with mmap=None\n",
            "23:17:08 : loading trainables recursively from wiki_attack_w2v.model.trainables.* with mmap=None\n",
            "23:17:08 : setting ignored attribute cum_table to None\n",
            "23:17:08 : loaded wiki_attack_w2v.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnevR1y9MVQZ"
      },
      "source": [
        "----------------------------\n",
        "As a quick, informal test of the quality of the model, let's look at some word comparisons that we think might appear in this dataset.\n",
        "\n",
        "Let's start by defining a helper function which will perform similarity searches using both models and then displays them side by side. That way we can apply it to a number of different words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiS-lhycMVQa"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_results(word):\n",
        "    '''\n",
        "    Performs similarity searches using both models and returns\n",
        "    a table showing them side-by-side.\n",
        "    '''\n",
        "\n",
        "    # Report the occurrence count for this word.\n",
        "    print(\"Word '%s' has %d samples in training text.\" % (word, model_ft.wv.vocab[word].count))\n",
        "\n",
        "    # Find the most similar words using both models.\n",
        "    print('Running similarity searches...')\n",
        "    results_ft = model_ft.wv.most_similar(word)\n",
        "    results_w2v = model_w2v.wv.most_similar(word)\n",
        "\n",
        "    # Merge the result into one table.\n",
        "    table_rows = []\n",
        "\n",
        "    # For each result...\n",
        "    for i in range(len(results_ft)):\n",
        "\n",
        "        # Get the words for result 'i'.\n",
        "        word_ft  =  results_ft[i][0]\n",
        "        word_w2v = results_w2v[i][0]\n",
        "\n",
        "        # Lookup the occurrence counts.\n",
        "        count_ft  = model_ft.wv.vocab[word_ft].count\n",
        "        count_w2v = model_ft.wv.vocab[word_w2v].count\n",
        "\n",
        "        score_ft  =  results_ft[i][1]\n",
        "        score_w2v = results_w2v[i][1]\n",
        "\n",
        "        # Combine result `i` from both models into to a single row.\n",
        "        # Format the similarity score to 2 decimal places.\n",
        "        table_rows.append(\n",
        "                            (word_ft,  '{:,}'.format(count_ft),  '{:.2}'.format(score_ft),\n",
        "                             word_w2v, '{:,}'.format(count_w2v), '{:.2}'.format(score_w2v))\n",
        "                         )\n",
        "\n",
        "    # Create a pandas dataframe to get a nice table display.\n",
        "    df = pd.DataFrame(table_rows, columns=['fasttext', 'freq', 'score', 'word2vec', 'freq', 'score'])\n",
        "    return(df)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6MPw0-EMVQe"
      },
      "source": [
        "----------------\n",
        "\n",
        "Let's start with the word 'condescending'. It occurs in our training text 84 times, so it should have a decent word vector.\n",
        "\n",
        "I think the results here are pretty fascinating!\n",
        "\n",
        "It's immediately apparent that fastText is able to make reasonable comparisons on words with relatively few samples. In particular, the misspelling 'condecending', and the conjugations 'condescendingly' and 'condescended'--*each of which only had three training samples*--are identified as strongly similar. Awesome!\n",
        "\n",
        "On the other hand, it's giving way too much weight to the words having overlap. The top two results, `descending` and `ascending`, are not good results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RCLtECjMVQf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "d193de0e-572f-4b46-dee8-43a7cfd72582"
      },
      "source": [
        "df = compare_results('condescending')\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23:17:09 : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word 'condescending' has 84 samples in training text.\n",
            "Running similarity searches...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fasttext</th>\n",
              "      <th>freq</th>\n",
              "      <th>score</th>\n",
              "      <th>word2vec</th>\n",
              "      <th>freq</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>descending</td>\n",
              "      <td>12</td>\n",
              "      <td>0.93</td>\n",
              "      <td>sarcastic</td>\n",
              "      <td>99</td>\n",
              "      <td>0.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ascending</td>\n",
              "      <td>7</td>\n",
              "      <td>0.91</td>\n",
              "      <td>aggressive</td>\n",
              "      <td>198</td>\n",
              "      <td>0.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>condecending</td>\n",
              "      <td>3</td>\n",
              "      <td>0.9</td>\n",
              "      <td>rude</td>\n",
              "      <td>516</td>\n",
              "      <td>0.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>condescendingly</td>\n",
              "      <td>3</td>\n",
              "      <td>0.89</td>\n",
              "      <td>uncivil</td>\n",
              "      <td>362</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>condescended</td>\n",
              "      <td>3</td>\n",
              "      <td>0.87</td>\n",
              "      <td>abusive</td>\n",
              "      <td>328</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>condemning</td>\n",
              "      <td>20</td>\n",
              "      <td>0.86</td>\n",
              "      <td>insulting</td>\n",
              "      <td>363</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>condoning</td>\n",
              "      <td>13</td>\n",
              "      <td>0.86</td>\n",
              "      <td>immature</td>\n",
              "      <td>152</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>disheartening</td>\n",
              "      <td>11</td>\n",
              "      <td>0.85</td>\n",
              "      <td>nasty</td>\n",
              "      <td>215</td>\n",
              "      <td>0.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>condensing</td>\n",
              "      <td>10</td>\n",
              "      <td>0.84</td>\n",
              "      <td>arrogant</td>\n",
              "      <td>252</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>malingering</td>\n",
              "      <td>4</td>\n",
              "      <td>0.84</td>\n",
              "      <td>polite</td>\n",
              "      <td>194</td>\n",
              "      <td>0.65</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          fasttext freq score    word2vec freq score\n",
              "0       descending   12  0.93   sarcastic   99  0.77\n",
              "1        ascending    7  0.91  aggressive  198  0.76\n",
              "2     condecending    3   0.9        rude  516  0.73\n",
              "3  condescendingly    3  0.89     uncivil  362  0.72\n",
              "4     condescended    3  0.87     abusive  328  0.67\n",
              "5       condemning   20  0.86   insulting  363  0.67\n",
              "6        condoning   13  0.86    immature  152  0.66\n",
              "7    disheartening   11  0.85       nasty  215  0.66\n",
              "8       condensing   10  0.84    arrogant  252  0.65\n",
              "9      malingering    4  0.84      polite  194  0.65"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tOpPj7xMVQl"
      },
      "source": [
        "---------------------------\n",
        "\n",
        "Here is a fun one to check out, I came across this word randomly in the vocabulary.\n",
        "\n",
        "Using subword information works exceptionally well here! Without subword info, 8 out of the 10 results are garbage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWaotKp-MVQm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "fda423d3-4250-40b7-da73-c9b2912e3b35"
      },
      "source": [
        "df = compare_results('hahahahahaha')\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 'hahahahahaha' has 14 samples in training text.\n",
            "Running similarity searches...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fasttext</th>\n",
              "      <th>freq</th>\n",
              "      <th>score</th>\n",
              "      <th>word2vec</th>\n",
              "      <th>freq</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hahahahahahaha</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>hahahaha</td>\n",
              "      <td>46</td>\n",
              "      <td>0.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hahahahaha</td>\n",
              "      <td>28</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ur</td>\n",
              "      <td>500</td>\n",
              "      <td>0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hahahaha</td>\n",
              "      <td>46</td>\n",
              "      <td>0.99</td>\n",
              "      <td>noob</td>\n",
              "      <td>47</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hahahahahah</td>\n",
              "      <td>3</td>\n",
              "      <td>0.99</td>\n",
              "      <td>xd</td>\n",
              "      <td>46</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ahahahahaha</td>\n",
              "      <td>3</td>\n",
              "      <td>0.98</td>\n",
              "      <td>beeblebrox</td>\n",
              "      <td>23</td>\n",
              "      <td>0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>bwahahahaha</td>\n",
              "      <td>2</td>\n",
              "      <td>0.98</td>\n",
              "      <td>gaey</td>\n",
              "      <td>2</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>hahahahah</td>\n",
              "      <td>4</td>\n",
              "      <td>0.98</td>\n",
              "      <td>fuckers</td>\n",
              "      <td>37</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ahahaha</td>\n",
              "      <td>3</td>\n",
              "      <td>0.97</td>\n",
              "      <td>dik</td>\n",
              "      <td>3</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>hahahah</td>\n",
              "      <td>9</td>\n",
              "      <td>0.97</td>\n",
              "      <td>tute</td>\n",
              "      <td>6</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>hahaha</td>\n",
              "      <td>124</td>\n",
              "      <td>0.96</td>\n",
              "      <td>nerd</td>\n",
              "      <td>139</td>\n",
              "      <td>0.67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         fasttext freq score    word2vec freq score\n",
              "0  hahahahahahaha    5   1.0    hahahaha   46  0.72\n",
              "1      hahahahaha   28   1.0          ur  500  0.69\n",
              "2        hahahaha   46  0.99        noob   47  0.68\n",
              "3     hahahahahah    3  0.99          xd   46  0.68\n",
              "4     ahahahahaha    3  0.98  beeblebrox   23  0.68\n",
              "5     bwahahahaha    2  0.98        gaey    2  0.67\n",
              "6       hahahahah    4  0.98     fuckers   37  0.67\n",
              "7         ahahaha    3  0.97         dik    3  0.67\n",
              "8         hahahah    9  0.97        tute    6  0.67\n",
              "9          hahaha  124  0.96        nerd  139  0.67"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf8cfeQ6MVQr"
      },
      "source": [
        "-----------------------------\n",
        "Let's look at two words which should be very similar, and are well represented in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_j0IkFMMVQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a862a971-16c1-4f71-fedd-164f67a60df0"
      },
      "source": [
        "print(\"Similarity between 'stupid' and 'dumb:'\")\n",
        "print(\"  fasttext: %.2f\" %  model_ft.wv.similarity('stupid', 'dumb'))\n",
        "print(\"  word2vec: %.2f\" % model_w2v.wv.similarity('stupid', 'dumb'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between 'stupid' and 'dumb:'\n",
            "  fasttext: 0.69\n",
            "  word2vec: 0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgXWMFYDMVQx"
      },
      "source": [
        "-----------------------------------------\n",
        "Note that another way to measure the \"likely quality\" of a word vector (besides looking at the training sample count) is to check out the vector's norm. Under-trained vectors tend to have low norms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-bz3j7EMVQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6aabde5-7ac5-496e-c822-bf65cc15b6d0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('%.3f' % np.linalg.norm(model_w2v.wv['stupid']))\n",
        "print('%.3f' % np.linalg.norm(model_w2v.wv['bwahahahaha']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16.118\n",
            "0.603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6HMRt9UMVQ3"
      },
      "source": [
        "# Appendix\n",
        "-------------------\n",
        "\n",
        "## Locate local gensim code\n",
        "---------------------------------------\n",
        "\n",
        "If you want to poke through your own local copy of the gensim functions, the following code can help you quickly locate the files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUU3LeAWMVQ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "239878ac-1222-477b-e775-101d131cd44e"
      },
      "source": [
        "import gensim\n",
        "import os\n",
        "\n",
        "path = gensim.models.__file__\n",
        "\n",
        "# On windows, un-escape the backslashes\n",
        "if os.name == 'nt':\n",
        "    path = path.replace('\\\\', '/')\n",
        "\n",
        "# Trim off __init__.py\n",
        "path = path[:-len('__init__.py')]\n",
        "\n",
        "# Add on the base file.\n",
        "path = path + 'base_any2vec.py'\n",
        "\n",
        "print(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}